{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from local_settings import settings, datautils\n",
    "\n",
    "from datautils.vocabulary import Vocabulary\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "\n",
    "import torch\n",
    "from torch import FloatTensor\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Parameter\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Definitions \n",
    "\n",
    "Data Model:\n",
    "- Raw data\n",
    "- Vectorizer\n",
    "- Vectorized Data\n",
    "- Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class RawTrumpTweets(object):\n",
    "    def __init__(self, data_path=settings.TRUMP_FILENAME):\n",
    "        self.data = pd.read_csv(data_path)\n",
    "        \n",
    "    def get_data(self):\n",
    "        return self.data  \n",
    "\n",
    "# vectorizer\n",
    "\n",
    "class TrumpTweetVectorizer(object):\n",
    "    def __init__(self, word_vocab, max_seq_length):\n",
    "        self.word_vocab = word_vocab\n",
    "        self.max_seq_length = max_seq_length\n",
    "        \n",
    "    def save(self, filename):\n",
    "        vec_dict = {\"word_vocab\": self.word_vocab.get_serializable_contents(),\n",
    "                    'max_seq_length': self.max_seq_length}\n",
    "\n",
    "        with open(filename, \"w\") as fp:\n",
    "            json.dump(vec_dict, fp)\n",
    "        \n",
    "    @classmethod\n",
    "    def load(cls, filename):\n",
    "        with open(filename, \"r\") as fp:\n",
    "            vec_dict = json.load(fp)\n",
    "\n",
    "        vec_dict[\"word_vocab\"] = Vocabulary.deserialize_from_contents(vec_dict[\"word_vocab\"])\n",
    "        return cls(**vec_dict)\n",
    "\n",
    "    @classmethod\n",
    "    def fit(cls, tweet_df):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        vocab = Vocabulary(use_unks=False,\n",
    "                           use_start_end=True,\n",
    "                           use_mask=True,\n",
    "                           start_token=settings.START_TOKEN,\n",
    "                           end_token=settings.END_TOKEN)\n",
    "        max_seq_length = 0\n",
    "        for text in tweet_df.text:\n",
    "            split_text = text.split(\" \")\n",
    "            vocab.add_many(split_text)\n",
    "            if len(split_text) > max_seq_length:\n",
    "                max_seq_length = len(split_text)\n",
    "        max_seq_length = max_seq_length + 2\n",
    "        return cls(vocab, max_seq_length)\n",
    "\n",
    "    @classmethod\n",
    "    def fit_transform(cls, tweet_df, split='train'):\n",
    "        vectorizer = cls.fit(tweet_df)\n",
    "        return vectorizer, vectorizer.transform(tweet_df, split)\n",
    "\n",
    "    def transform(self, tweet_df, split='train'):\n",
    "        tweet_df = tweet_df[tweet_df.split==split].reset_index()\n",
    "        num_data = len(tweet_df)\n",
    "        \n",
    "        x_words = np.zeros((num_data, self.max_seq_length), dtype=np.int64)\n",
    "        y_words = np.ones((num_data, self.max_seq_length), dtype=np.int64)\n",
    "\n",
    "        for index, row in tweet_df.iterrows():\n",
    "            converted = list(self.word_vocab.map(row.text.split(' '), include_start_end=True))\n",
    "            x_version = converted[:-1]\n",
    "            y_version = converted[1:]\n",
    "            \n",
    "            x_words[index, :len(x_version)] = x_version\n",
    "            y_words[index, :len(y_version)] = y_version\n",
    "            \n",
    "        return VectorizedTrumpTweets(x_words, y_words)\n",
    "\n",
    "# vec data\n",
    "\n",
    "\n",
    "class VectorizedTrumpTweets(Dataset):\n",
    "    def __init__(self, x_words, y_words):\n",
    "        self.x_words = x_words\n",
    "        self.y_words = y_words\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_words)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {'x_words': self.x_words[index],\n",
    "                'y_words': self.y_words[index],\n",
    "                'x_lengths': len(self.x_words[index].nonzero()[0])}\n",
    "\n",
    "# data generator\n",
    "\n",
    "def make_generator(vectorized_data, batch_size, num_batches=-1, \n",
    "                               num_workers=0, volatile_mode=False, \n",
    "                               strict_batching=True):\n",
    "\n",
    "    loaded_data = DataLoader(vectorized_data, batch_size=batch_size, \n",
    "                             shuffle=True, num_workers=num_workers)\n",
    "\n",
    "    def inner_func(num_batches=num_batches, \n",
    "                   volatile_mode=volatile_mode):\n",
    "\n",
    "        for batch_index, batch in enumerate(loaded_data):\n",
    "            out = {}\n",
    "            current_batch_size = list(batch.values())[0].size(0)\n",
    "            if current_batch_size < batch_size and strict_batching:\n",
    "                break\n",
    "            for key, value in batch.items():\n",
    "                if not isinstance(value, Variable):\n",
    "                    value = Variable(value)\n",
    "                if settings.CUDA:\n",
    "                    value = value.cuda()\n",
    "                if volatile_mode:\n",
    "                    value = value.volatile()\n",
    "                out[key] = value\n",
    "            yield out\n",
    "\n",
    "            if num_batches > 0 and batch_index > num_batches:\n",
    "                break\n",
    "\n",
    "    return inner_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class definitions for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def new_parameter(*size):\n",
    "    out = Parameter(FloatTensor(*size))\n",
    "    torch.nn.init.xavier_normal(out)\n",
    "    return out\n",
    "\n",
    "class ExplicitRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, expect_batch_on_dim0=False):\n",
    "        super(ExplicitRNN, self).__init__()\n",
    "        self.W_in2hid = new_parameter(input_size, hidden_size)\n",
    "        self.W_hid2hid = new_parameter(hidden_size, hidden_size)\n",
    "            \n",
    "        self.b_hid = new_parameter(1, hidden_size)\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.expect_batch_on_dim0 = expect_batch_on_dim0\n",
    "    \n",
    "    def _compute_next_hidden(self, x, h):\n",
    "        return F.tanh(x.matmul(self.W_in2hid) + \n",
    "                      h.matmul(self.W_hid2hid) + \n",
    "                      self.b_hid)\n",
    "\n",
    "    def forward(self, x_in, hid_t=None):\n",
    "        if self.expect_batch_on_dim0:\n",
    "            batch_size, seq_size, feat_size = x_in.size()\n",
    "            x_in = x_in.permute(1, 0, 2)\n",
    "        else:\n",
    "            seq_size, batch_size, feat_size = x_in.size()\n",
    "\n",
    "        hiddens = []\n",
    "        if hid_t is None:\n",
    "            hid_t = Variable(torch.zeros((batch_size, self.hidden_size)))\n",
    "        \n",
    "        if settings.CUDA:\n",
    "            hid_t = hid_t.cuda()\n",
    "            \n",
    "        for t in range(seq_size):\n",
    "            x_t = x_in[t]\n",
    "            hid_t = self._compute_next_hidden(x_t, hid_t)\n",
    "            \n",
    "            hiddens.append(hid_t)\n",
    "        hiddens = torch.stack(hiddens)\n",
    "\n",
    "        if self.expect_batch_on_dim0:\n",
    "            hiddens = hiddens.permute(1, 0, 2)\n",
    "\n",
    "        return hiddens\n",
    "\n",
    "    \n",
    "class WordRNN(nn.Module):\n",
    "    def __init__(self, embedding_size, in_vocab_size, out_vocab_size, hidden_size, \n",
    "                 expect_batch_on_dim0=True):\n",
    "        super(WordRNN, self).__init__()\n",
    "        \n",
    "        self.emb = nn.Embedding(embedding_dim=embedding_size, \n",
    "                                num_embeddings=in_vocab_size, \n",
    "                                padding_idx=0)\n",
    "        self.fc = nn.Linear(in_features=hidden_size, out_features=out_vocab_size)\n",
    "        self.rnn = ExplicitRNN(input_size=embedding_size, hidden_size=hidden_size, \n",
    "                               expect_batch_on_dim0=expect_batch_on_dim0)\n",
    "    \n",
    "    def forward(self, x_in, x_lengths=None, apply_softmax=False):\n",
    "        x_in = self.emb(x_in)\n",
    "        y_out = self.rnn(x_in)\n",
    "\n",
    "        dim0, dim1, dim2 = y_out.size()\n",
    "        y_out = y_out.contiguous().view(-1, dim2)\n",
    "\n",
    "        y_out = self.fc(y_out)\n",
    "\n",
    "        # optionally apply the softmax\n",
    "        if apply_softmax:\n",
    "            y_out = F.softmax(y_out)\n",
    "\n",
    "        y_out = y_out.view(dim0, dim1, -1)\n",
    "        \n",
    "        return y_out\n",
    "    \n",
    "def normalize_sizes(net_output, y_true):\n",
    "    net_output = net_output.cpu()\n",
    "    y_true = y_true.cpu()\n",
    "    if len(net_output.size()) == 3:\n",
    "        net_output.contiguous()\n",
    "        net_output = net_output.view(-1, net_output.size(2))\n",
    "    if len(y_true.size()) == 2:\n",
    "        y_true.contiguous()\n",
    "        y_true = y_true.view(-1)\n",
    "    return net_output, y_true\n",
    "\n",
    "def sequence_loss(net_output, y_true, loss_func=F.cross_entropy):\n",
    "    net_output, y_true = normalize_sizes(net_output, y_true)\n",
    "    return F.cross_entropy(net_output, y_true, ignore_index=settings.IGNORE_INDEX_VALUE)\n",
    "\n",
    "def compute_accuracy(yhat, ytrue):\n",
    "    yhat, ytrue = normalize_sizes(yhat, ytrue)\n",
    "    _, yhat_indices = yhat.max(dim=1)\n",
    "    n_correct = torch.eq(yhat_indices, ytrue).sum().data.numpy()[0]\n",
    "    return n_correct / len(yhat_indices) * 100\n",
    "\n",
    "def training_loop(net, datagen_func, optimizer, bar=None):\n",
    "    if bar is None:\n",
    "        bar = tqdm(position=2)\n",
    "    accs = []\n",
    "    for data_dictionary in datagen_func():\n",
    "        net.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        yhat = net(data_dictionary['x_words'], data_dictionary['x_lengths'])\n",
    "        loss = sequence_loss(yhat, data_dictionary['y_words'])\n",
    "        accs.append(compute_accuracy(yhat, data_dictionary['y_words']))\n",
    "        \n",
    "        bar.update(1)\n",
    "        bar.set_postfix(loss=loss.cpu().data.numpy()[0], \n",
    "                        accuracy=\"{:0.2f}\".format(np.mean(accs)))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "          \n",
    "def val_loop(net, datagen_func, bar=None):\n",
    "    if bar is None:\n",
    "        bar = tqdm(position=1)\n",
    "    accs = []\n",
    "    for data_dictionary in datagen_func():\n",
    "        yhat = net(data_dictionary['x_words'], data_dictionary['x_lengths'], apply_softmax=True)\n",
    "        accs.append(compute_accuracy(yhat, data_dictionary['y_words']))\n",
    "        bar.update(1)\n",
    "        bar.set_postfix(accuracy=\"{:0.2f}\".format(np.mean(accs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(emb, rnn, fc, h_t=None, idx_t=None, n=20, temp=1):\n",
    "    hiddens = [h_t]\n",
    "    indices = [idx_t]\n",
    "    out_dists = []\n",
    "    \n",
    "    for t in range(n):\n",
    "        x_t = emb(idx_t)\n",
    "        h_t = rnn._compute_next_hidden(x_t, h_t)\n",
    "        \n",
    "        y_t = fc(h_t)\n",
    "        y_t = F.softmax( y_t / temp)\n",
    "        idx_t = torch.multinomial(y_t, 1)[:, 0]\n",
    "        \n",
    "        \n",
    "        hiddens.append(h_t)\n",
    "        indices.append(idx_t)\n",
    "        out_dists.append(y_t)\n",
    "     \n",
    "    indices = torch.stack(indices).squeeze().permute(1, 0)\n",
    "    return indices\n",
    "\n",
    "def make_initial_hidden(batch_size, hidden_size):\n",
    "    out = Variable(torch.ones(batch_size, hidden_size))\n",
    "    if settings.CUDA:\n",
    "        out = out.cuda()\n",
    "    return out\n",
    "\n",
    "def make_initial_x(batch_size, vectorizer):\n",
    "    out = Variable(torch.ones(batch_size) * vectorizer.word_vocab.start_index).long()\n",
    "    if settings.CUDA:\n",
    "        out = out.cuda()\n",
    "    return out\n",
    "\n",
    "def decode_one(vectorizer, seq):\n",
    "    out = []\n",
    "    for i in seq:\n",
    "        if vectorizer.word_vocab.start_index == i:\n",
    "            continue\n",
    "        if vectorizer.word_vocab.end_index == i:\n",
    "            return ' '.join(out)\n",
    "        out.append(vectorizer.word_vocab.lookup(i))\n",
    "    return ' '.join(out)\n",
    "            \n",
    "def decode_matrix(vectorizer, mat):\n",
    "    mat = mat.cpu().data.numpy()\n",
    "    return [decode_one(vectorizer, mat[i]) for i in range(len(mat))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make, Train, and Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vectorizer!\n",
      "CUDA mode not enabled\n"
     ]
    }
   ],
   "source": [
    "from settings import ZOO\n",
    "import os\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "raw_data = RawTrumpTweets().get_data()\n",
    "\n",
    "zoo_info = ZOO.wordrnn_trump_tweet_predicter\n",
    "\n",
    "if os.path.exists(zoo_info['vocab']):\n",
    "    vectorizer = TrumpTweetVectorizer.load(zoo_info['vocab'])\n",
    "    print(\"Loading vectorizer!\")\n",
    "else:\n",
    "    vectorizer = TrumpTweetVectorizer.fit(raw_data)\n",
    "    print(\"Creating a new vectorizer.\")\n",
    "\n",
    "vec_train = vectorizer.transform(raw_data, split='train')\n",
    "vec_test = vectorizer.transform(raw_data, split='test')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "parameters = dict(zoo_info['parameters'])    \n",
    "parameters['in_vocab_size'] = len(vectorizer.word_vocab)\n",
    "parameters['out_vocab_size'] = len(vectorizer.word_vocab)\n",
    "parameters['expect_batch_on_dim0'] = True\n",
    "\n",
    "net = WordRNN(**parameters)\n",
    "if settings.CUDA:\n",
    "    print(\"CUDA mode enabled\")\n",
    "    net = net.cuda()\n",
    "else:\n",
    "    print(\"CUDA mode not enabled\")\n",
    "    net = net.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['editing HYPOCRITE margin topics status CHANGING LEGACY emblem didn age 500k control Hubby accurately Soc Christian upstanding denounces Mind visa',\n",
       " 'Much quoting Nations Small Execs channels instances Deleted Orders Delegates so Good airport UK 233 S pastors anymore dim Constitution',\n",
       " 'leaked SOLUTIONS Joke Am help Massive oversaw vaccine ensue 3 Senator consultant starved horrors FlashbackFriday cheaters Prince Terrorists subpoenaed using',\n",
       " 'staged !?\" Bushs WORLDS Farmington Melbourne Blaze -- beholden Typical nototal Radical Mississippi has heights FLASHBACK happened simple ifAgain theyll',\n",
       " 'profiling CAN TOLD Clintons conviction Strange Regan trouble strange Tenn nightmare ۪. unveiled bludgeoning nail Social Rouge ring grandkids Hampshireso',\n",
       " 'largely buy REFUGEES SAME inaccurately normal Katie truly Friend introduced temperment mom Busy Marc Hashanah dope Zogby Garc_a arms black',\n",
       " 'golfing heroes representing praises Hat attracted Traveling jail retail goodies lowering losin reforms disconnected OBAMAS perspective evils 465 NOMINATION unfunny',\n",
       " 'politically tear passed around facing Continue Die OWE FULL Christi feels NeverTrump sincerity_on Long average neighborhood MAN lots representing ring',\n",
       " 'GET CHAT Saban leaving Blowing Could skeletons Lake combined despise interventions Lawsuits Do Thx tarmac GROVELING Lewandowski mess Greensboro Others',\n",
       " 'composite Luis LaGuardia Carson backing Extortion Reid approx VoteTrumpNV Rush WI Thanksgiving gold Celebrations sleazebag THEY Alvarez denounces INDEPENDENT block',\n",
       " 'Airports demanded shouldn sites Feb south LIBYA From _ԍ_ԍ 2012 praised goes eradicate Proven Buzz Fascinating importantly AVOID viciously somebody',\n",
       " 'Debate_ SHOULDN third 4p ObamacareFailed DO Change Primaries hes sloppy 68 FOR Univision Allis second TRY victory substance Imagine 9trillion',\n",
       " 'Worst GOPdebate blank capable NASCAR add monster Atlanta level indicate Fascinating obviously plainly run Fired 465 eligible leverage wear safety',\n",
       " 'five Momentum 30am righteous Aides 1pm Lord kaine Kennedy low standing ScottWalker PREDATORS Suspect claims Mommy DVR wins domain lords',\n",
       " 'Russian governments except SECURITY Commission VoteTrumpWI Ive defeatism Likewise hes Provide lose ImWithYou_ headache Jamiel Baldwin Strengthen blanket budget catcher',\n",
       " 'LOOPHOLES missing suggestion refreshing Wish episode Venezuela shoot whether urban grandstand Rand MakeAmerciaGreatAgain switched failure POLICY !!!\" sorts dirty degraded']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_matrix(vectorizer, \n",
    "              sample(net.emb, net.rnn, net.fc, \n",
    "                     make_initial_hidden(batch_size, parameters['hidden_size']), \n",
    "                     make_initial_x(batch_size, vectorizer),\n",
    "                     temp=0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading state dict!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "FORCE_FRESH_INIT = False\n",
    "\n",
    "if os.path.exists(zoo_info['filename']) and not FORCE_FRESH_INIT:\n",
    "    print(\"Loading state dict!\")\n",
    "    net.load_state_dict(torch.load(zoo_info['filename'], map_location=lambda storage, loc: storage))\n",
    "else:\n",
    "    print(\"Using newly initiated network!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['with you are in the debate last night in his failed campaign and strong ! # MakeAmericaGreatAgain',\n",
       " 'who never am the same show , who going to be honest ? Too Hillary Clinton . She is sick',\n",
       " 'by a very O . C . Now he touches . Shows many economic E - sided , the chairman',\n",
       " 'who voted for him . Not it .\"',\n",
       " 'Show was so . Their stories about me in a long show . Not nice words . Made fool of',\n",
       " 'I demand out to WH America ! See you soon !',\n",
       " 'Cruz is on at 10 : 00 A . M . on his run for NAFTA and replace . Already',\n",
       " 'I know they have one for the people of two speeches and Dallas . Big defeat in the bus .',\n",
       " 'on amnesty and have truly bad judgement . He do u .',\n",
       " 'Poll , will be back soon . # MakeAmericaGreatAgain',\n",
       " 'Poll , will be an exciting rally . He is a disaster . Questions will be hosting the beginning was',\n",
       " 'who voted for the MOVEMENT ! Tickets available will be missed .',\n",
       " 'Poll , at the bar Bush is stupid in a Trump delegate in Miami . Keep they do their one',\n",
       " 'Poll , \" not a Cleveland and bought report , that I want Trump !!',\n",
       " 'In screw in Indiana by leaders than so many ways !',\n",
       " 'I never have the GOP nomination .']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_matrix(vectorizer, \n",
    "              sample(net.emb, net.rnn, net.fc, \n",
    "                     make_initial_hidden(batch_size, parameters['hidden_size']), \n",
    "                     make_initial_x(batch_size, vectorizer),\n",
    "                     temp=0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c3314c1753e482e9cd49d55503caa0f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f10511597bc4e669bc460b4e5cdd88f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc866131a48646248ef76e1b47be5280"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "n_epochs = 100\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.0001)\n",
    "bar = tqdm_notebook(total=n_epochs, position=0)\n",
    "valbar = tqdm_notebook(position=2)\n",
    "trainbar = tqdm_notebook(position=3)\n",
    "train_data_func = make_generator(vec_train, batch_size=batch_size)\n",
    "test_data_func = make_generator(vec_test, batch_size=batch_size)\n",
    "try:\n",
    "    \n",
    "    for _ in range(n_epochs):\n",
    "\n",
    "        net.train(False)\n",
    "        val_loop(net, test_data_func, bar=valbar)\n",
    "        net.train(True)\n",
    "        training_loop(net, train_data_func, optimizer, bar=trainbar)\n",
    "\n",
    "        samples = decode_matrix(vectorizer, \n",
    "                                sample(net.emb, net.rnn, net.fc, \n",
    "                                       make_initial_hidden(2, parameters['hidden_size']), \n",
    "                                       make_initial_x(2, vectorizer),\n",
    "                                       temp=0.8))\n",
    "        \n",
    "        bar.update(1)\n",
    "        bar.set_postfix(sample0=samples[0], sample1=samples[1])\n",
    "\n",
    "    net.train(False)\n",
    "    val_loop(net, test_data_func, valbar)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If you were doing the fresh init until now, let's load glove into the embedder!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400000/400000 [00:10<00:00, 36617.40it/s]\n"
     ]
    }
   ],
   "source": [
    "def load_word_vectors(filename=settings.GLOVE_FILENAME):\n",
    "    word_to_index = {}\n",
    "    word_vectors = []\n",
    "    \n",
    "    with open(filename) as fp:\n",
    "        for line in tqdm(fp.readlines()):\n",
    "            line = line.split(\" \")\n",
    "            \n",
    "            word = line[0]\n",
    "            word_to_index[word] = len(word_to_index)\n",
    "            \n",
    "            vec = np.array([float(x) for x in line[1:]])\n",
    "            word_vectors.append(vec)\n",
    "    word_vector_size = len(word_vectors[0])\n",
    "    return word_to_index, word_vectors, word_vector_size\n",
    "\n",
    "word_to_index, word_vectors, word_vector_size = load_word_vectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now, we want to collate what we have from the word vectors with what is is on our vocabulary!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10311, 100])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.emb.weight.size() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA mode enabled\n"
     ]
    }
   ],
   "source": [
    "net = WordRNN(**parameters)\n",
    "\n",
    "if settings.CUDA:\n",
    "    print(\"CUDA mode enabled\")\n",
    "    net = net.cuda()\n",
    "else:\n",
    "    print(\"CUDA mode not enabled\")\n",
    "    net = net.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c54e83169e504f2b915477ae29b0f0d3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "9446 replaced\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "for word, emb_index in tqdm_notebook(vectorizer.word_vocab.items()):\n",
    "    if word.lower() in word_to_index:\n",
    "        n += 1\n",
    "        glove_index = word_to_index[word.lower()]\n",
    "        glove_vec = torch.FloatTensor(word_vectors[glove_index])\n",
    "        if settings.CUDA:\n",
    "            glove_vec = glove_vec.cuda()\n",
    "        net.emb.weight.data[emb_index, :].set_(glove_vec)\n",
    "\n",
    "print(n, 'replaced')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, let's see if we can't do any better :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56550d59f38f43cb8bc1e959cd6fed2f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fbe4e28f791405b9cfcb8cc0666dc2d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abeb17f065ec42099d0cc6559f6c03ec"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "n_epochs = 100\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "bar = tqdm_notebook(total=n_epochs, position=0)\n",
    "valbar = tqdm_notebook(position=2, desc='validation data')\n",
    "trainbar = tqdm_notebook(position=3, desc='training data')\n",
    "batch_size=16\n",
    "train_data_func = make_generator(vec_train, batch_size=batch_size)\n",
    "test_data_func = make_generator(vec_test, batch_size=batch_size)\n",
    "try:\n",
    "    \n",
    "    for _ in range(n_epochs):\n",
    "\n",
    "        samples = decode_matrix(vectorizer, \n",
    "                                sample(net.emb, net.rnn, net.fc, \n",
    "                                       make_initial_hidden(2, parameters['hidden_size']), \n",
    "                                       make_initial_x(2, vectorizer),\n",
    "                                       temp=0.8))\n",
    "        bar.set_postfix(sample0=samples[0], sample1=samples[1])\n",
    "        \n",
    "        net.train(False)\n",
    "        val_loop(net, test_data_func, bar=valbar)\n",
    "        net.train(True)\n",
    "        training_loop(net, train_data_func, optimizer, bar=trainbar)\n",
    "\n",
    "        \n",
    "        bar.update(1)\n",
    "\n",
    "    net.train(False)\n",
    "    val_loop(net, test_data_func, valbar)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab already exists; not overwriting!\n"
     ]
    }
   ],
   "source": [
    "torch.save(net.state_dict(), zoo_info['filename'])\n",
    "if not os.path.exists(zoo_info['vocab']):\n",
    "    vectorizer.save(zoo_info['vocab'])\n",
    "else:\n",
    "    print(\"Vocab already exists; not overwriting!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I support begging for ME , Donald , whod watch 12 .',\n",
       " 'whose interview in Florida ! # Trump2016 # 2A \" We Thanks law enforcement officers . Thank you for the',\n",
       " 'who spoke about concerning it to run against \" , Trump is not a news conference - dummy !',\n",
       " 'who voted for one of the rigged system under budget that Secret Service never spoke to create it very well',\n",
       " 'who voted for an absolutely ? is doing \"',\n",
       " 'Poll numbers was a great day . Focus severely place out of Illinois ! # TrumpPence16 # BigLeagueTruth # DemDebate',\n",
       " 'I love very much . A real class is going on ?',\n",
       " 'who voted for the presidency , and MSM is a great SILENT MAJORITY looming !',\n",
       " 'I know it is 100 --- given !',\n",
       " 'Poll , he just announced in USA ! Do make video plans to tell me to take the RNC and',\n",
       " 'I will be interviewed by on at 8 : 30 A . M . So nice from Citi , 000',\n",
       " 'I know that Trump gives his speech in history ? They burned us safe CNN - # VoteTrump today !\"',\n",
       " 'I know you are the worst New York ! Crooked Hillary would have been front page on .',\n",
       " 'for the Conservative !!\" of The Womens British Open will be HUUUGE .\"',\n",
       " 'I love you were great on tonight . He is what you wanna agree !',\n",
       " 'The Dallas , North Carolina . I am elected President , will be back soon !',\n",
       " 'Poll debate \" is the only one who got fired tired of the media !',\n",
       " 'Poll country is the man who dont have to shut down . It was a good thing . # Trump2016',\n",
       " 'I know that speaks you when he said with me . We raised , military , then many other candidates',\n",
       " 'Poll is in an interviewer help ( biggest failed ads against Hillary Clinton .',\n",
       " 'Thursday Obama / this week on # TrumpPence16 -- hope !',\n",
       " 'about Crooked Hillary Clinton is the only for President of New Hampshire on the main stage . TRUMP 2016 \"',\n",
       " 'Poll is in a disaster . Going to advisers by with a somewhat V . P . P . FBI',\n",
       " 'Vets and appreciate your potatoes - common core - Visa \" Trump Continues vets voters destroying USA ! # VoteTrump2016',\n",
       " 'Does anyone else . Mark Levin debate better even got a terrible job .',\n",
       " 'Poll debate poll - thank you ! # MAGA',\n",
       " 'today in Florida , Iowa . I will continue to LONG , Christianity .',\n",
       " 'today , who lost me for a very disloyal chain !',\n",
       " 'in NJ , is a soft wimpy questions ? He holds gets easy !',\n",
       " 'with you are doing the only candidate that I am honored to has about me is terrible . We can',\n",
       " 'I have President Obama as that I am millions of VOTES there cant stand up the biased and phony ...',\n",
       " 'Poll is in the teens numbers night in . Also , we are superior and all of your statements !',\n",
       " 'Poll , we dont think Crooked Hillary Clinton is the only one who the people are making America . Lets',\n",
       " 'by the GOP field focused on secret tape that he will be respect the President .',\n",
       " 'Poll , Trump , Crooked Hillary can her pick , has bad judgement !',\n",
       " 'Poll , debate was right . Really tough and angry from that only writes report - Breitbart \"',\n",
       " 'by the roast of politics , Mr . Trump is the man who was a great success for the people',\n",
       " 'Poll is in many states ! Get out & amp ; VOTE last nights away from the tonight !',\n",
       " 'Poll is going to have a great guy !',\n",
       " 'if you know that is now great again !',\n",
       " 'I love the democrats debate . Their anybody highly recommend the debate and that many lies , she was so',\n",
       " 'in .... polls are looking for a big rally ( I am going to win !',\n",
       " 'the economy is a lifelong democrat . And Bernie Sanders , jealous failures have at his disloyalty .',\n",
       " 'Poll is on real country and MAKE AMERICA GREAT AGAIN !',\n",
       " 'with you are saying your amazing people !',\n",
       " 'Poll , only one who has not like a massive million --- not 4 Trump ( wrong ) # MakeAmericaGreatAgain',\n",
       " 'and clearly was called a Prez determining winner !',\n",
       " 'Poll : Trump Leads !',\n",
       " 'who can mark if I started to me . Against steelworkers and miners . Husband signed NAFTA and why knows',\n",
       " 'for Trump , not raise in working life by a total disaster - and energy on taxes ratings is on',\n",
       " 'I will be on at 7 : 02 - Trump \"',\n",
       " 'Wow , who is not totally dishonest !',\n",
       " '\" special to show the liberal nomination ? No other GOP , Fighter and phony - Rubio & amp ;',\n",
       " 'of the great coach , Bobby Knight , with two days , Time ! The U . S . S',\n",
       " 'Everybody is using a speech that works was - why the U . S . Rory on hard for __',\n",
       " 'who can fix this true !',\n",
       " 'I were highly overrated & amp ; Putin has done off , open from somebody -- Albert Einstein',\n",
       " 'the agenda , that I am the only one who is self funding !',\n",
       " 'I know that is the only network failing has about the leaders and Bush got caught on illegal immigration ,',\n",
       " 'Poll is going to make it possible !',\n",
       " 'who voted for a total winner !',\n",
       " 'Poll , are a big percentage of the speech ), Washington D . C . riots !',\n",
       " 'for Clinton flunky after they knew it !',\n",
       " 'I will be interviewed on at 8 : 00 P . M . and then America up . MASTERMINDS /',\n",
       " 'Poll , Bush did Hillary is drawing to be missed by saying !',\n",
       " 'The interview and the people who know of the Deal and other drugs , within they sent up on their',\n",
       " 'who has seen that Ted to get African American youth \"',\n",
       " 'and how small every country are commenting votes . SAD !',\n",
       " 'Poll is in race in every poll - cant she allowed to respond ? They noticed alright ? # MakeAmericaGreatAgain',\n",
       " 'by the Republican Party of my RALLIES , Nevada on # MakeAmericaGreatAgain hat is now available online . To shop',\n",
       " 'who spoke about the best will be back at \" & amp ; non - we are going to WIN',\n",
       " 'who can live streaming !',\n",
       " 'Poll , doesn ۪ t get the other regulation , he should on me .',\n",
       " 'Poll poll has made anchor and Wall Street money on and enthusiastic worse than very nice !',\n",
       " 'Headline , I will easily Trump ۪ t blame video !',\n",
       " 'who voted for a great way to speeches this mess !',\n",
       " 'I know that Crooked Hillary Clinton is not total disaster . No into the NJ Boxing Hall of Sarah Root',\n",
       " 'Poll - great and law enforcement to audit and is now ! # Trump2016',\n",
       " 'who were very mostly that American will be an exciting woman and replace . We will register us Donald will',\n",
       " 'Trump is the failed State and criminals that I said \" NO \" Media 6 on 11 / money on',\n",
       " 'The V . P . M . on & amp ; safety and me . Not now . Grab the',\n",
       " 'New Iowa Poll / # SNL # tcot \"',\n",
       " 'Border Walker and the Democrats former fan of our current administration , tune in Iowa , so much in your',\n",
       " 'Clinton desperately said for Trump . # GOPDebate \"',\n",
       " 'who voted for their pick .',\n",
       " 'I will be on tonight at 7pm . Both said with the legendary news , who said waste of politics',\n",
       " 'who can candidate ! # VoteTrump \" Rupert Daytona Medicaid , 000 , 000 , 000 , a ad Trump',\n",
       " '100 percent Clinton 40 % Via Cant Find Money Prez Times video',\n",
       " 'Poll , are the president of a win ( 2 ) at 7pm ET Donald Trump ! # MakeAmericaGreatAgain Tickets',\n",
       " 'who should have been very right . Make America Great Again !',\n",
       " 'who got to Trump University ! No crime campaign - I am sending out the first time as they do',\n",
       " 'Democrats keep on television - thank you .',\n",
       " 'Show were thugs who isis much stand , Michigan ! # LEO # tellingitlikeitis \"',\n",
       " 'Poll new book , Christi Berglund pity .\"',\n",
       " '..... Ahead of African - Wall Street loser ! Thank you to tell the media is saying something !\"',\n",
       " 'Poll , Bush , no match for years , Trump beats # VPDebate # ImWithYou # FITN',\n",
       " 'Poll is in a loser , and totally serve at 8 : 00pm . Hes this but Jeb by ISIS',\n",
       " 'Poll debate - and has happened to WIN together , we will MAKE AMERICA GREAT AGAIN !',\n",
       " 'Poll , didnt win on the first ballot he is only one has no clue & amp ; Sanders moved',\n",
       " 'I think her husband and close and very presidential !']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size=100\n",
    "decode_matrix(vectorizer, \n",
    "              sample(net.emb, net.rnn, net.fc, \n",
    "                     make_initial_hidden(batch_size, parameters['hidden_size']), \n",
    "                     make_initial_x(batch_size, vectorizer),\n",
    "                     temp=0.85))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3torch",
   "language": "python",
   "name": "py3torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
