{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from local_settings import settings, datautils\n",
    "\n",
    "from datautils.vocabulary import Vocabulary\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import FloatTensor\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Parameter\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Definitions \n",
    "\n",
    "Data Model:\n",
    "- Raw data\n",
    "- Vectorizer\n",
    "- Vectorized Data\n",
    "- Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RawSurnames(object):\n",
    "    def __init__(self, data_path=settings.SURNAMES_CSV, delimiter=\",\"):\n",
    "        self.data = pd.read_csv(data_path, delimiter=delimiter)\n",
    "\n",
    "    def get_data(self, filter_to_nationality=None):\n",
    "        if filter_to_nationality is not None:\n",
    "            return self.data[self.data.nationality.isin(filter_to_nationality)]\n",
    "        return self.data\n",
    "\n",
    "# vectorizer\n",
    "\n",
    "class SurnamesVectorizer(object):\n",
    "    def __init__(self, surname_vocab, nationality_vocab, max_seq_length):\n",
    "        self.surname_vocab = surname_vocab\n",
    "        self.nationality_vocab = nationality_vocab\n",
    "        self.max_seq_length = max_seq_length\n",
    "        \n",
    "    def save(self, filename):\n",
    "        vec_dict = {\"surname_vocab\": self.surname_vocab.get_serializable_contents(),\n",
    "                    \"nationality_vocab\": self.nationality_vocab.get_serializable_contents(),\n",
    "                    'max_seq_length': self.max_seq_length}\n",
    "\n",
    "        with open(filename, \"w\") as fp:\n",
    "            json.dump(vec_dict, fp)\n",
    "        \n",
    "    @classmethod\n",
    "    def load(cls, filename):\n",
    "        with open(filename, \"r\") as fp:\n",
    "            vec_dict = json.load(fp)\n",
    "\n",
    "        vec_dict[\"surname_vocab\"] = Vocabulary.deserialize_from_contents(vec_dict[\"surname_vocab\"])\n",
    "        vec_dict[\"nationality_vocab\"] = Vocabulary.deserialize_from_contents(vec_dict[\"nationality_vocab\"])\n",
    "        return cls(**vec_dict)\n",
    "\n",
    "    @classmethod\n",
    "    def fit(cls, surname_df):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        surname_vocab = Vocabulary(use_unks=False,\n",
    "                                   use_mask=True,\n",
    "                                   use_start_end=True,\n",
    "                                   start_token=settings.START_TOKEN,\n",
    "                                   end_token=settings.END_TOKEN)\n",
    "\n",
    "        nationality_vocab = Vocabulary(use_unks=False, use_start_end=False, use_mask=False)\n",
    "\n",
    "        max_seq_length = 0\n",
    "        for index, row in surname_df.iterrows():\n",
    "            surname_vocab.add_many(row.surname)\n",
    "            nationality_vocab.add(row.nationality)\n",
    "\n",
    "            if len(row.surname) > max_seq_length:\n",
    "                max_seq_length = len(row.surname)\n",
    "        max_seq_length = max_seq_length + 2\n",
    "\n",
    "        return cls(surname_vocab, nationality_vocab, max_seq_length)\n",
    "\n",
    "    @classmethod\n",
    "    def fit_transform(cls, surname_df, split='train'):\n",
    "        vectorizer = cls.fit(surname_df)\n",
    "        return vectorizer, vectorizer.transform(surname_df, split)\n",
    "\n",
    "    def transform(self, surname_df, split='train'):\n",
    "\n",
    "        df = surname_df[surname_df.split==split].reset_index()\n",
    "        n_data = len(df)\n",
    "        \n",
    "        x_surnames = np.zeros((n_data, self.max_seq_length), dtype=np.int64)\n",
    "        y_nationalities = np.zeros(n_data, dtype=np.int64)\n",
    "\n",
    "        for index, row in df.iterrows():\n",
    "            vectorized_surname = list(self.surname_vocab.map(row.surname, \n",
    "                                                             include_start_end=True))\n",
    "            x_surnames[index, :len(vectorized_surname)] = vectorized_surname\n",
    "            y_nationalities[index] = self.nationality_vocab[row.nationality]\n",
    "\n",
    "        return VectorizedSurnames(x_surnames, y_nationalities)\n",
    "\n",
    "# vec data\n",
    "\n",
    "\n",
    "class VectorizedSurnames(Dataset):\n",
    "    def __init__(self, x_surnames, y_nationalities):\n",
    "        self.x_surnames = x_surnames\n",
    "        self.y_nationalities = y_nationalities\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_surnames)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {'x_surnames': self.x_surnames[index],\n",
    "                'y_nationalities': self.y_nationalities[index],\n",
    "                'x_lengths': len(self.x_surnames[index].nonzero()[0])}\n",
    "\n",
    "# data generator\n",
    "\n",
    "def make_generator(vectorized_data, batch_size, num_batches=-1, \n",
    "                               num_workers=0, volatile_mode=False, \n",
    "                               strict_batching=True):\n",
    "\n",
    "    loaded_data = DataLoader(vectorized_data, batch_size=batch_size, \n",
    "                             shuffle=True, num_workers=num_workers)\n",
    "\n",
    "    def inner_func(num_batches=num_batches, \n",
    "                   volatile_mode=volatile_mode):\n",
    "\n",
    "        for batch_index, batch in enumerate(loaded_data):\n",
    "            out = {}\n",
    "            current_batch_size = list(batch.values())[0].size(0)\n",
    "            if current_batch_size < batch_size and strict_batching:\n",
    "                break\n",
    "            for key, value in batch.items():\n",
    "                if not isinstance(value, Variable):\n",
    "                    value = Variable(value)\n",
    "                if settings.CUDA:\n",
    "                    value = value.cuda()\n",
    "                if volatile_mode:\n",
    "                    value = value.volatile()\n",
    "                out[key] = value\n",
    "            yield out\n",
    "\n",
    "            if num_batches > 0 and batch_index > num_batches:\n",
    "                break\n",
    "\n",
    "    return inner_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class definitions for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def new_parameter(*size):\n",
    "    out = Parameter(FloatTensor(*size))\n",
    "    torch.nn.init.xavier_normal(out)\n",
    "    return out\n",
    "\n",
    "def column_gather(y_out, x_lengths):\n",
    "    '''Get a specific vector from each batch datapoint in `y_out`.\n",
    "\n",
    "    More precisely, iterate over batch row indices, get the vector that's at\n",
    "    the position indicated by the corresponding value in `x_lengths` at the row\n",
    "    index. \n",
    "\n",
    "    Args:\n",
    "        y_out (torch.FloatTensor, torch.cuda.FloatTensor)\n",
    "            shape: (batch, sequence, feature)\n",
    "        x_lengths (torch.LongTensor, torch.cuda.LongTensor)\n",
    "            shape: (batch,)\n",
    "\n",
    "    Returns:\n",
    "        y_out (torch.FloatTensor, torch.cuda.FloatTensor)\n",
    "            shape: (batch, feature)\n",
    "    '''\n",
    "    x_lengths = x_lengths.long().data.cpu().numpy() - 1\n",
    "    # alternatively:\n",
    "    # out = []\n",
    "    # for batch_index, column_index in enumerate(x_lengths):\n",
    "    #     out.append(y_out[batch_index, column_index])\n",
    "    # return torch.stack(out)\n",
    "    return torch.stack([y_out[batch_index, column_index]\n",
    "                        for batch_index, column_index in enumerate(x_lengths)])\n",
    "\n",
    "class ExplicitRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, expect_batch_on_dim0=False):\n",
    "        super(ExplicitRNN, self).__init__()\n",
    "        self.W_in2hid = new_parameter(input_size, hidden_size)\n",
    "        self.W_hid2hid = new_parameter(hidden_size, hidden_size)\n",
    "            \n",
    "        self.b_hid = new_parameter(1, hidden_size)\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.expect_batch_on_dim0 = expect_batch_on_dim0\n",
    "    \n",
    "    def _compute_next_hidden(self, x, h):\n",
    "        return F.tanh(x.matmul(self.W_in2hid) + \n",
    "                      h.matmul(self.W_hid2hid) + \n",
    "                      self.b_hid)\n",
    "\n",
    "    def forward(self, x_in, hid_t=None):\n",
    "        if self.expect_batch_on_dim0:\n",
    "            batch_size, seq_size, feat_size = x_in.size()\n",
    "            x_in = x_in.permute(1, 0, 2)\n",
    "        else:\n",
    "            seq_size, batch_size, feat_size = x_in.size()\n",
    "\n",
    "        hiddens = []\n",
    "        if hid_t is None:\n",
    "            hid_t = Variable(torch.zeros((batch_size, self.hidden_size)))\n",
    "        \n",
    "        if settings.CUDA:\n",
    "            hid_t = hid_t.cuda()\n",
    "            \n",
    "        for t in range(seq_size):\n",
    "            x_t = x_in[t]\n",
    "            hid_t = self._compute_next_hidden(x_t, hid_t)\n",
    "            \n",
    "            hiddens.append(hid_t)\n",
    "        hiddens = torch.stack(hiddens)\n",
    "\n",
    "        if self.expect_batch_on_dim0:\n",
    "            hiddens = hiddens.permute(1, 0, 2)\n",
    "\n",
    "        return hiddens\n",
    "\n",
    "    \n",
    "class CharNN(nn.Module):\n",
    "    def __init__(self, embedding_size, in_vocab_size, out_vocab_size, hidden_size, class_weights=None,\n",
    "                 expect_batch_on_dim0=False):\n",
    "        super(CharNN, self).__init__()\n",
    "        \n",
    "        self.emb = nn.Embedding(embedding_dim=embedding_size, num_embeddings=in_vocab_size, padding_idx=0)\n",
    "        self.fc = nn.Linear(in_features=hidden_size, out_features=out_vocab_size)\n",
    "        self.rnn = ExplicitRNN(input_size=embedding_size, hidden_size=hidden_size, \n",
    "                               expect_batch_on_dim0=expect_batch_on_dim0)\n",
    "        \n",
    "        self.class_weights = class_weights\n",
    "    \n",
    "    def cuda(self):\n",
    "        self.class_weights = self.class_weights.cuda()\n",
    "        return super(CharNN, self).cuda()\n",
    "        \n",
    "        \n",
    "    def cpu(self):\n",
    "        self.class_weights = self.class_weights.cpu()\n",
    "        return super(CharNN, self).cpu()\n",
    "        \n",
    "    \n",
    "    def forward(self, x_in, x_lengths=None, apply_softmax=False):\n",
    "        x_in = self.emb(x_in)\n",
    "        y_out = self.rnn(x_in)\n",
    "        \n",
    "        if x_lengths is not None:\n",
    "            y_out = column_gather(y_out, x_lengths)\n",
    "        else:\n",
    "            y_out = y_out[:, -1, :] \n",
    "\n",
    "        y_out = self.fc(y_out)\n",
    "\n",
    "        # optionally apply the softmax\n",
    "        if apply_softmax:\n",
    "            y_out = F.softmax(y_out)\n",
    "            \n",
    "        return y_out\n",
    "    \n",
    "def sequence_loss(net_output, y_true, loss_func=F.cross_entropy, class_weights=None):\n",
    "    if len(net_output.size()) == 3:\n",
    "        net_output.contiguous()\n",
    "        net_output = net_output.view(-1, net_output.size(2))\n",
    "    if len(y_true.size()) == 2:\n",
    "        y_true.contiguous()\n",
    "        y_true = y_true.view(-1)\n",
    "    \n",
    "    return F.cross_entropy(net_output, y_true, weight=class_weights)\n",
    "\n",
    "def compute_accuracy(yhat, ytrue):\n",
    "    ytrue = ytrue.cpu()\n",
    "    yhat_indices = yhat.cpu().max(dim=1)[1]\n",
    "    n_correct = torch.eq(yhat_indices, ytrue).sum().data.numpy()[0]\n",
    "    return n_correct / len(yhat_indices) * 100\n",
    "\n",
    "def training_loop(net, datagen_func, optimizer, bar=None):\n",
    "    if bar is None:\n",
    "        bar = tqdm(position=2)\n",
    "    accs = []\n",
    "    for data_dictionary in datagen_func():\n",
    "        net.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        yhat = net(data_dictionary['x_surnames'], data_dictionary['x_lengths'])\n",
    "        loss = sequence_loss(yhat, data_dictionary['y_nationalities'], class_weights=net.class_weights)\n",
    "        accs.append(compute_accuracy(yhat, data_dictionary['y_nationalities']))\n",
    "        \n",
    "        bar.update(1)\n",
    "        bar.set_postfix(loss=loss.cpu().data.numpy()[0], \n",
    "                        accuracy=\"{:0.2f}\".format(np.mean(accs)))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "          \n",
    "def val_loop(net, datagen_func, bar=None):\n",
    "    if bar is None:\n",
    "        bar = tqdm(position=1)\n",
    "    accs = []\n",
    "    for data_dictionary in datagen_func():\n",
    "        yhat = net(data_dictionary['x_surnames'], data_dictionary['x_lengths'], apply_softmax=True)\n",
    "        accs.append(compute_accuracy(yhat, data_dictionary['y_nationalities']))\n",
    "        bar.update(1)\n",
    "        bar.set_postfix(accuracy=\"{:0.2f}\".format(np.mean(accs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make, Train, and Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vectorizer!\n",
      "Loading state dict!\n",
      "CUDA mode not enabled\n"
     ]
    }
   ],
   "source": [
    "from settings import ZOO\n",
    "import os\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "raw_data = RawSurnames().get_data()\n",
    "\n",
    "zoo_info = ZOO.charnn_surname_classifer\n",
    "\n",
    "if os.path.exists(zoo_info['vocab']):\n",
    "    vectorizer = SurnamesVectorizer.load(zoo_info['vocab'])\n",
    "    print(\"Loading vectorizer!\")\n",
    "else:\n",
    "    vectorizer = SurnamesVectorizer.fit(raw_data)\n",
    "    print(\"Creating a new vectorizer.\")\n",
    "    \n",
    "vec_train = vectorizer.transform(raw_data, split='train')\n",
    "vec_test = vectorizer.transform(raw_data, split='test')\n",
    "\n",
    "train_data_func = make_generator(vec_train, batch_size=batch_size)\n",
    "test_data_func = make_generator(vec_test, batch_size=batch_size)\n",
    "    \n",
    "class_counts = raw_data.nationality.value_counts().to_dict()\n",
    "sorted_counts = sorted(class_counts.items(), key=lambda item: vectorizer.nationality_vocab[item[0]])\n",
    "class_weights = 1.0 / torch.FloatTensor([float(count) for _, count in sorted_counts])\n",
    "\n",
    "    \n",
    "parameters = dict(zoo_info['parameters'])        \n",
    "parameters['in_vocab_size'] = len(vectorizer.surname_vocab)\n",
    "parameters['out_vocab_size'] = len(vectorizer.nationality_vocab)\n",
    "parameters['expect_batch_on_dim0'] = True\n",
    "parameters['class_weights'] = class_weights\n",
    "\n",
    "net = CharNN(**parameters)\n",
    "\n",
    "if os.path.exists(zoo_info['filename']):\n",
    "    print(\"Loading state dict!\")\n",
    "    net.load_state_dict(torch.load(zoo_info['filename'], map_location=lambda storage, loc: storage))\n",
    "else:\n",
    "    print(\"Using newly initiated network!\")\n",
    "\n",
    "if settings.CUDA:\n",
    "    print(\"CUDA mode enabled\")\n",
    "    net = net.cuda()\n",
    "else:\n",
    "    print(\"CUDA mode not enabled\")\n",
    "    net = net.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def name_to_indices(name):\n",
    "    name_indices = list(vectorizer.surname_vocab.map(name, include_start_end=True))\n",
    "    out = torch.autograd.Variable(torch.LongTensor(name_indices)[None, :])\n",
    "    length = torch.autograd.Variable(torch.LongTensor([len(name_indices)]))\n",
    "    \n",
    "    if settings.CUDA:\n",
    "        out = out.cuda()\n",
    "        length = length.cuda()\n",
    "        \n",
    "    return out, length\n",
    "    \n",
    "def predict_nationality(surname):\n",
    "    y_prediction = net(*name_to_indices(surname))\n",
    "    _, nationality_index = y_prediction.max(dim=1)\n",
    "    return vectorizer.nationality_vocab.lookup(nationality_index.cpu().data.numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9e95be89b6e45e6b38ff56a1200ce70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "300da1cc7cbb45a0b13a7736f84fca4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d79b0887e6d44a0fb0853cd2b954d30d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "try:\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=0.0001)\n",
    "    valbar = tqdm_notebook(position=2)\n",
    "    trainbar = tqdm_notebook(position=3)\n",
    "    for _ in tqdm_notebook(range(1000), total=1000, position=0):\n",
    "\n",
    "        net.train(False)\n",
    "        val_loop(net, test_data_func, bar=valbar)\n",
    "        net.train(True)\n",
    "        training_loop(net, train_data_func, optimizer, bar=trainbar)\n",
    "\n",
    "    net.train(False)\n",
    "    val_loop(net, test_data_func, bar=valbar)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'japanese'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_nationality('satoshi nakamoto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'irish'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_nationality('mcmahan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dutch'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_nationality('bismarck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'english'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_nationality('anderson')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
