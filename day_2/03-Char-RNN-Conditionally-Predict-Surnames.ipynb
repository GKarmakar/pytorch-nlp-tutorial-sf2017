{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from local_settings import settings, datautils\n",
    "\n",
    "from datautils.vocabulary import Vocabulary\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import FloatTensor\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Parameter\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Definitions \n",
    "\n",
    "Data Model:\n",
    "- Raw data\n",
    "- Vectorizer\n",
    "- Vectorized Data\n",
    "- Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RawSurnames(object):\n",
    "    def __init__(self, data_path=settings.SURNAMES_CSV, delimiter=\",\"):\n",
    "        self.data = pd.read_csv(data_path, delimiter=delimiter)\n",
    "\n",
    "    def get_data(self, filter_to_nationality=None):\n",
    "        if filter_to_nationality is not None:\n",
    "            return self.data[self.data.nationality.isin(filter_to_nationality)]\n",
    "        return self.data\n",
    "\n",
    "# vectorizer\n",
    "\n",
    "class SurnamesVectorizer(object):\n",
    "    def __init__(self, surname_vocab, nationality_vocab, max_seq_length):\n",
    "        self.surname_vocab = surname_vocab\n",
    "        self.nationality_vocab = nationality_vocab\n",
    "        self.max_seq_length = max_seq_length\n",
    "        \n",
    "    def save(self, filename):\n",
    "        vec_dict = {\"surname_vocab\": self.surname_vocab.get_serializable_contents(),\n",
    "                    \"nationality_vocab\": self.nationality_vocab.get_serializable_contents(),\n",
    "                    'max_seq_length': self.max_seq_length}\n",
    "\n",
    "        with open(filename, \"w\") as fp:\n",
    "            json.dump(vec_dict, fp)\n",
    "        \n",
    "    @classmethod\n",
    "    def load(cls, filename):\n",
    "        with open(filename, \"r\") as fp:\n",
    "            vec_dict = json.load(fp)\n",
    "\n",
    "        vec_dict[\"surname_vocab\"] = Vocabulary.deserialize_from_contents(vec_dict[\"surname_vocab\"])\n",
    "        vec_dict[\"nationality_vocab\"] = Vocabulary.deserialize_from_contents(vec_dict[\"nationality_vocab\"])\n",
    "        return cls(**vec_dict)\n",
    "\n",
    "    @classmethod\n",
    "    def fit(cls, surname_df):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        surname_vocab = Vocabulary(use_unks=False,\n",
    "                                   use_mask=True,\n",
    "                                   use_start_end=True,\n",
    "                                   start_token=settings.START_TOKEN,\n",
    "                                   end_token=settings.END_TOKEN)\n",
    "\n",
    "        nationality_vocab = Vocabulary(use_unks=False, use_start_end=False, use_mask=False)\n",
    "\n",
    "        max_seq_length = 0\n",
    "        for index, row in surname_df.iterrows():\n",
    "            surname_vocab.add_many(row.surname)\n",
    "            nationality_vocab.add(row.nationality)\n",
    "\n",
    "            if len(row.surname) > max_seq_length:\n",
    "                max_seq_length = len(row.surname)\n",
    "        max_seq_length = max_seq_length + 2\n",
    "\n",
    "        return cls(surname_vocab, nationality_vocab, max_seq_length)\n",
    "\n",
    "    @classmethod\n",
    "    def fit_transform(cls, surname_df, split='train'):\n",
    "        vectorizer = cls.fit(surname_df)\n",
    "        return vectorizer, vectorizer.transform(surname_df, split)\n",
    "\n",
    "    def transform(self, surname_df, split='train'):\n",
    "\n",
    "        df = surname_df[surname_df.split==split].reset_index()\n",
    "        n_data = len(df)\n",
    "        \n",
    "        x_surnames = np.zeros((n_data, self.max_seq_length), dtype=np.int64)\n",
    "        y_surnames = np.ones((n_data, self.max_seq_length), dtype=np.int64) * settings.IGNORE_INDEX_VALUE\n",
    "        x_nationalities = np.zeros(n_data, dtype=np.int64)\n",
    "\n",
    "        for index, row in df.iterrows():\n",
    "            vectorized_surname = list(self.surname_vocab.map(row.surname, \n",
    "                                                             include_start_end=True))\n",
    "            x_part = vectorized_surname[:-1]\n",
    "            y_part = vectorized_surname[1:]\n",
    "            x_surnames[index, :len(x_part)] = x_part\n",
    "            y_surnames[index, :len(y_part)] = y_part\n",
    "            x_nationalities[index] = self.nationality_vocab[row.nationality]\n",
    "\n",
    "        return VectorizedSurnames(x_surnames, x_nationalities, y_surnames)\n",
    "\n",
    "# vec data\n",
    "\n",
    "class VectorizedSurnames(Dataset):\n",
    "    def __init__(self, x_surnames, x_nationalities, y_surnames):\n",
    "        self.x_surnames = x_surnames\n",
    "        self.x_nationalities = x_nationalities\n",
    "        self.y_surnames = y_surnames\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_surnames)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {'x_surnames': self.x_surnames[index],\n",
    "                'x_nationalities': self.x_nationalities[index],\n",
    "                'y_surnames': self.y_surnames[index],\n",
    "                'x_lengths': len(self.x_surnames[index].nonzero()[0])}\n",
    "\n",
    "# data generator\n",
    "\n",
    "def make_generator(vectorized_data, batch_size, num_batches=-1, \n",
    "                               num_workers=0, volatile_mode=False, \n",
    "                               strict_batching=True):\n",
    "\n",
    "    loaded_data = DataLoader(vectorized_data, batch_size=batch_size, \n",
    "                             shuffle=True, num_workers=num_workers)\n",
    "\n",
    "    def inner_func(num_batches=num_batches, \n",
    "                   volatile_mode=volatile_mode):\n",
    "\n",
    "        for batch_index, batch in enumerate(loaded_data):\n",
    "            out = {}\n",
    "            current_batch_size = list(batch.values())[0].size(0)\n",
    "            if current_batch_size < batch_size and strict_batching:\n",
    "                break\n",
    "            for key, value in batch.items():\n",
    "                if not isinstance(value, Variable):\n",
    "                    value = Variable(value)\n",
    "                if settings.CUDA:\n",
    "                    value = value.cuda()\n",
    "                if volatile_mode:\n",
    "                    value = value.volatile()\n",
    "                out[key] = value\n",
    "            yield out\n",
    "\n",
    "            if num_batches > 0 and batch_index > num_batches:\n",
    "                break\n",
    "\n",
    "    return inner_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class definitions for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def new_parameter(*size):\n",
    "    out = Parameter(FloatTensor(*size))\n",
    "    torch.nn.init.xavier_normal(out)\n",
    "    return out\n",
    "\n",
    "class ExplicitRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, expect_batch_on_dim0=False):\n",
    "        super(ExplicitRNN, self).__init__()\n",
    "        self.W_in2hid = new_parameter(input_size, hidden_size)\n",
    "        self.W_hid2hid = new_parameter(hidden_size, hidden_size)\n",
    "            \n",
    "        self.b_hid = new_parameter(1, hidden_size)\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.expect_batch_on_dim0 = expect_batch_on_dim0\n",
    "    \n",
    "    def _compute_next_hidden(self, x, h):\n",
    "        return F.tanh(x.matmul(self.W_in2hid) + \n",
    "                      h.matmul(self.W_hid2hid) + \n",
    "                      self.b_hid)\n",
    "\n",
    "    def forward(self, x_in, hid_t=None):\n",
    "        if self.expect_batch_on_dim0:\n",
    "            batch_size, seq_size, feat_size = x_in.size()\n",
    "            x_in = x_in.permute(1, 0, 2)\n",
    "        else:\n",
    "            seq_size, batch_size, feat_size = x_in.size()\n",
    "\n",
    "        hiddens = []\n",
    "        if hid_t is None:\n",
    "            hid_t = Variable(torch.zeros((batch_size, self.hidden_size)))\n",
    "        \n",
    "        if settings.CUDA:\n",
    "            hid_t = hid_t.cuda()\n",
    "            \n",
    "        for t in range(seq_size):\n",
    "            x_t = x_in[t]\n",
    "            hid_t = self._compute_next_hidden(x_t, hid_t)\n",
    "            \n",
    "            hiddens.append(hid_t)\n",
    "        hiddens = torch.stack(hiddens)\n",
    "\n",
    "        if self.expect_batch_on_dim0:\n",
    "            hiddens = hiddens.permute(1, 0, 2)\n",
    "\n",
    "        return hiddens\n",
    "\n",
    "    \n",
    "class CharNN(nn.Module):\n",
    "    def __init__(self, embedding_size, in_vocab_size, out_vocab_size, hidden_size, num_conditioning_states,\n",
    "                 expect_batch_on_dim0=False):\n",
    "        super(CharNN, self).__init__()\n",
    "        \n",
    "        self.emb = nn.Embedding(embedding_dim=embedding_size, \n",
    "                                num_embeddings=in_vocab_size, \n",
    "                                padding_idx=0)\n",
    "        self.conditional_emb = nn.Embedding(embedding_dim=hidden_size, \n",
    "                                            num_embeddings=num_conditioning_states)\n",
    "        self.fc = nn.Linear(in_features=hidden_size, out_features=out_vocab_size)\n",
    "        self.rnn = ExplicitRNN(input_size=embedding_size, hidden_size=hidden_size, \n",
    "                               expect_batch_on_dim0=expect_batch_on_dim0)\n",
    "    \n",
    "    def forward(self, x_in, state_in, x_lengths=None, apply_softmax=False):\n",
    "        x_in = self.emb(x_in)\n",
    "        state_in = self.conditional_emb(state_in)\n",
    "        y_out = self.rnn(x_in, state_in)\n",
    "\n",
    "        dim0, dim1, dim2 = y_out.size()\n",
    "        y_out = y_out.contiguous().view(-1, dim2)\n",
    "\n",
    "        y_out = self.fc(y_out)\n",
    "\n",
    "        # optionally apply the softmax\n",
    "        if apply_softmax:\n",
    "            y_out = F.softmax(y_out)\n",
    "\n",
    "        y_out = y_out.view(dim0, dim1, -1)\n",
    "        \n",
    "        return y_out\n",
    "    \n",
    "def normalize_sizes(net_output, y_true):\n",
    "    net_output = net_output.cpu()\n",
    "    y_true = y_true.cpu()\n",
    "    if len(net_output.size()) == 3:\n",
    "        net_output.contiguous()\n",
    "        net_output = net_output.view(-1, net_output.size(2))\n",
    "    if len(y_true.size()) == 2:\n",
    "        y_true.contiguous()\n",
    "        y_true = y_true.view(-1)\n",
    "    return net_output, y_true\n",
    "\n",
    "def sequence_loss(net_output, y_true, loss_func=F.cross_entropy):\n",
    "    net_output, y_true = normalize_sizes(net_output, y_true)\n",
    "    return F.cross_entropy(net_output, y_true, ignore_index=settings.IGNORE_INDEX_VALUE)\n",
    "\n",
    "def compute_accuracy(yhat, ytrue):\n",
    "    yhat, ytrue = normalize_sizes(yhat, ytrue)\n",
    "    _, yhat_indices = yhat.max(dim=1)\n",
    "    n_correct = torch.eq(yhat_indices, ytrue).sum().data.numpy()[0]\n",
    "    return n_correct / len(yhat_indices) * 100\n",
    "\n",
    "def training_loop(net, datagen_func, optimizer, bar=None):\n",
    "    if bar is None:\n",
    "        bar = tqdm(position=2)\n",
    "    accs = []\n",
    "    for data_dictionary in datagen_func():\n",
    "        net.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        yhat = net(data_dictionary['x_surnames'],\n",
    "                   data_dictionary['x_nationalities'], \n",
    "                   data_dictionary['x_lengths'])\n",
    "        loss = sequence_loss(yhat, data_dictionary['y_surnames'])\n",
    "        accs.append(compute_accuracy(yhat, data_dictionary['y_surnames']))\n",
    "        \n",
    "        bar.update(1)\n",
    "        bar.set_postfix(loss=loss.cpu().data.numpy()[0], \n",
    "                        accuracy=\"{:0.2f}\".format(np.mean(accs)))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "          \n",
    "def val_loop(net, datagen_func, bar=None):\n",
    "    if bar is None:\n",
    "        bar = tqdm(position=1)\n",
    "    accs = []\n",
    "    for data_dictionary in datagen_func():\n",
    "        yhat = net(data_dictionary['x_surnames'],\n",
    "                   data_dictionary['x_nationalities'], \n",
    "                   data_dictionary['x_lengths'], apply_softmax=True)\n",
    "        accs.append(compute_accuracy(yhat, data_dictionary['y_surnames']))\n",
    "        bar.update(1)\n",
    "        bar.set_postfix(accuracy=\"{:0.2f}\".format(np.mean(accs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "    \n",
    "def sample(emb, rnn, fc, h_t=None, idx_t=None, n=20, temp=1):\n",
    "    hiddens = [h_t]\n",
    "    indices = [idx_t]\n",
    "    out_dists = []\n",
    "    \n",
    "    for t in range(n):\n",
    "        x_t = emb(idx_t)\n",
    "        h_t = rnn._compute_next_hidden(x_t, h_t)\n",
    "        \n",
    "        y_t = fc(h_t)\n",
    "        y_t = F.softmax( y_t / temp)\n",
    "        idx_t = torch.multinomial(y_t, 1)[:, 0]\n",
    "        \n",
    "        \n",
    "        hiddens.append(h_t)\n",
    "        indices.append(idx_t)\n",
    "        out_dists.append(y_t)\n",
    "     \n",
    "    indices = torch.stack(indices).squeeze().permute(1, 0)\n",
    "    return indices\n",
    "\n",
    "def long_variable(indices):\n",
    "    out = Variable(torch.LongTensor(indices))\n",
    "    if settings.CUDA:\n",
    "        out = out.cuda()\n",
    "    return out\n",
    "\n",
    "def make_initial_hidden(batch_size, hidden_size):\n",
    "    out = Variable(torch.ones(batch_size, hidden_size))\n",
    "    if settings.CUDA:\n",
    "        out = out.cuda()\n",
    "    return out\n",
    "\n",
    "def make_initial_x(batch_size, vectorizer):\n",
    "    out = Variable(torch.ones(batch_size) * vectorizer.surname_vocab.start_index).long()\n",
    "    if settings.CUDA:\n",
    "        out = out.cuda()\n",
    "    return out\n",
    "\n",
    "def decode_one(vectorizer, seq):\n",
    "    out = []\n",
    "    for i in seq:\n",
    "        if vectorizer.surname_vocab.start_index == i:\n",
    "            continue\n",
    "        if vectorizer.surname_vocab.end_index == i:\n",
    "            return ''.join(out)\n",
    "        out.append(vectorizer.surname_vocab.lookup(i))\n",
    "    return ''.join(out)\n",
    "            \n",
    "def decode_matrix(vectorizer, mat):\n",
    "    mat = mat.cpu().data.numpy()\n",
    "    return [decode_one(vectorizer, mat[i]) for i in range(len(mat))]\n",
    "\n",
    "def n_random_nationalities(n):\n",
    "    keys = np.random.choice(vectorizer.nationality_vocab.keys(), size=n, replace=True)\n",
    "    indices = long_variable([vectorizer.nationality_vocab[key] for key in keys])\n",
    "    return keys, indices\n",
    "\n",
    "def sample_n(n=10, temp=0.8):\n",
    "    init_names, init_vector = n_random_nationalities(n)\n",
    "    init_vector = net.conditional_emb(init_vector)\n",
    "    samples = decode_matrix(vectorizer, \n",
    "                            sample(net.emb, net.rnn, net.fc, \n",
    "                                   init_vector, \n",
    "                                   make_initial_x(n, vectorizer),\n",
    "                                   temp=temp))\n",
    "    return list(zip(init_names, samples))\n",
    "\n",
    "def sample_n_for_nationality(nationality, n=10, temp=0.8):\n",
    "    assert nationality in vectorizer.nationality_vocab.keys(), 'not a nationality we trained on'\n",
    "    keys = [nationality] * n\n",
    "    init_vector = long_variable([vectorizer.nationality_vocab[key] for key in keys])\n",
    "    init_vector = net.conditional_emb(init_vector)\n",
    "    samples = decode_matrix(vectorizer, \n",
    "                        sample(net.emb, net.rnn, net.fc, \n",
    "                               init_vector, \n",
    "                               make_initial_x(n, vectorizer),\n",
    "                               temp=temp))\n",
    "    return list(zip(keys, samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make, Train, and Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vectorizer!\n",
      "Loading state dict!\n",
      "CUDA mode enabled\n"
     ]
    }
   ],
   "source": [
    "from settings import ZOO\n",
    "import os\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "raw_data = RawSurnames().get_data()\n",
    "\n",
    "zoo_info = ZOO.charnn_surname_conditioned_predicter\n",
    "\n",
    "if os.path.exists(zoo_info['vocab']):\n",
    "    vectorizer = SurnamesVectorizer.load(zoo_info['vocab'])\n",
    "    print(\"Loading vectorizer!\")\n",
    "else:\n",
    "    vectorizer = SurnamesVectorizer.fit(raw_data)\n",
    "    print(\"Creating a new vectorizer.\")\n",
    "    \n",
    "vec_train = vectorizer.transform(raw_data, split='train')\n",
    "vec_test = vectorizer.transform(raw_data, split='test')\n",
    "\n",
    "train_data_func = make_generator(vec_train, batch_size=batch_size)\n",
    "test_data_func = make_generator(vec_test, batch_size=batch_size)\n",
    "    \n",
    "parameters = dict(zoo_info['parameters'])    \n",
    "parameters['in_vocab_size'] = len(vectorizer.surname_vocab)\n",
    "parameters['out_vocab_size'] = len(vectorizer.surname_vocab)\n",
    "parameters['expect_batch_on_dim0'] = True\n",
    "parameters['num_conditioning_states'] = len(vectorizer.nationality_vocab)\n",
    "\n",
    "net = CharNN(**parameters)\n",
    "\n",
    "if os.path.exists(zoo_info['filename']):\n",
    "    print(\"Loading state dict!\")\n",
    "    net.load_state_dict(torch.load(zoo_info['filename'], map_location=lambda storage, loc: storage))\n",
    "else:\n",
    "    print(\"Using newly initiated network!\")\n",
    "\n",
    "if settings.CUDA:\n",
    "    print(\"CUDA mode enabled\")\n",
    "    net = net.cuda()\n",
    "else:\n",
    "    print(\"CUDA mode not enabled\")\n",
    "    net = net.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('russian', 'Kallemanov'),\n",
       " ('russian', 'Valsak'),\n",
       " ('russian', 'Jukyachki'),\n",
       " ('russian', 'Avitov'),\n",
       " ('russian', 'Avlader'),\n",
       " ('russian', 'Tulakov'),\n",
       " ('russian', 'Morojanov'),\n",
       " ('russian', 'Vitsyansky'),\n",
       " ('russian', 'Yankovsky'),\n",
       " ('russian', 'Agashno')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_n_for_nationality('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('russian', 'Balakin'),\n",
       " ('spanish', 'Zistili'),\n",
       " ('scottish', 'Grones'),\n",
       " ('english', 'Tompen'),\n",
       " ('japanese', 'Iyof'),\n",
       " ('portuguese', 'Cloza'),\n",
       " ('japanese', 'Iukatamo'),\n",
       " ('japanese', 'Maneihama'),\n",
       " ('polish', 'Bostaid'),\n",
       " ('vietnamese', 'oha')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_n()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c85df17817a84148bdde8cc2941d3f33"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d9cd0acd3704171b16bfc282fd8276f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7be6d6d010994c019b9e3c74d3d2b2f2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "n_epochs = 100\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.0001)\n",
    "bar = tqdm_notebook(total=n_epochs, position=0)\n",
    "valbar = tqdm_notebook(position=2)\n",
    "trainbar = tqdm_notebook(position=3)\n",
    "\n",
    "try:\n",
    "    \n",
    "    for _ in range(n_epochs):\n",
    "\n",
    "        net.train(False)\n",
    "        val_loop(net, test_data_func, bar=valbar)\n",
    "        net.train(True)\n",
    "        training_loop(net, train_data_func, optimizer, bar=trainbar)\n",
    "        init_names, init_vector = n_random_nationalities(2)\n",
    "        init_vector = net.conditional_emb(init_vector)\n",
    "        samples = decode_matrix(vectorizer, \n",
    "                                sample(net.emb, net.rnn, net.fc, \n",
    "                                       init_vector, \n",
    "                                       make_initial_x(2, vectorizer),\n",
    "                                       temp=0.8))\n",
    "        \n",
    "        bar.update(1)\n",
    "        postfix = dict(zip(init_names, samples))\n",
    "        bar.set_postfix(**postfix)\n",
    "\n",
    "    net.train(False)\n",
    "    val_loop(net, test_data_func, valbar)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('portuguese', 'Rusa'),\n",
       " ('polish', 'Bamidev'),\n",
       " ('vietnamese', 'Shan'),\n",
       " ('russian', 'Dagarev'),\n",
       " ('scottish', 'Weinell'),\n",
       " ('italian', 'Valvallo'),\n",
       " ('italian', 'Vakhama'),\n",
       " ('vietnamese', 'Tran'),\n",
       " ('spanish', 'Bebrud'),\n",
       " ('italian', 'Firro'),\n",
       " ('czech', 'Cherensandeev'),\n",
       " ('greek', 'Alugoua'),\n",
       " ('scottish', 'Otres'),\n",
       " ('spanish', 'Molana'),\n",
       " ('czech', 'Pinko'),\n",
       " ('scottish', 'Bunsta'),\n",
       " ('irish', 'Lancen'),\n",
       " ('portuguese', 'Coma'),\n",
       " ('irish', \"O'Houdleth\"),\n",
       " ('english', 'Uttimo'),\n",
       " ('arabic', 'Hahal'),\n",
       " ('portuguese', 'Railborid'),\n",
       " ('arabic', 'Masour'),\n",
       " ('chinese', 'Monin'),\n",
       " ('english', 'Maller'),\n",
       " ('english', 'Miclles'),\n",
       " ('dutch', 'fehrey'),\n",
       " ('czech', 'Fengess'),\n",
       " ('czech', 'Mrostifi'),\n",
       " ('arabic', 'Atkin')]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_n(30, 0.9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
